# Rainbow DQN Implementation for Space Invaders

A complete, from-scratch implementation of Rainbow DQN (Deep Q-Network with all improvements) trained on Atari Space Invaders. This project features clean architecture, comprehensive logging, and easy-to-use scripts for training and evaluation.

## ğŸŒˆ What is Rainbow DQN?

Rainbow DQN combines seven extensions to the original DQN algorithm:

1. **Double Q-Learning** - Reduces overestimation of Q-values
2. **Prioritized Experience Replay** - Samples important transitions more frequently
3. **Dueling Networks** - Separate value and advantage streams
4. **Multi-step Learning** - Uses n-step returns for faster learning
5. **Distributional RL (C51)** - Learns distribution of returns instead of expected value
6. **Noisy Networks** - Learned exploration without epsilon-greedy
7. **Target Network** - Stabilizes training with periodic updates

## ğŸ“ Project Structure

```
RLCoursework/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ agents/                   # Agent implementations
â”‚   â”‚   â””â”€â”€ rainbow_agent.py      # Rainbow DQN agent
â”‚   â”œâ”€â”€ networks/                 # Neural network architectures
â”‚   â”‚   â”œâ”€â”€ noisy_linear.py       # Noisy linear layer
â”‚   â”‚   â””â”€â”€ rainbow_network.py    # Rainbow DQN network
â”‚   â”œâ”€â”€ environment/              # Environment wrappers
â”‚   â”‚   â””â”€â”€ atari_wrappers.py     # Atari preprocessing
â”‚   â””â”€â”€ utils/                    # Utility functions
â”‚       â”œâ”€â”€ replay_buffer.py      # Prioritized replay buffer
â”‚       â”œâ”€â”€ n_step.py             # N-step return calculator
â”‚       â””â”€â”€ logger.py             # Logging utilities
â”œâ”€â”€ configs/                      # Configuration files
â”‚   â””â”€â”€ space_invaders_config.py  # Hyperparameters
â”œâ”€â”€ scripts/                      # Executable scripts
â”‚   â”œâ”€â”€ train.py                  # Training script
â”‚   â”œâ”€â”€ evaluate.py               # Evaluation script
â”‚   â”œâ”€â”€ watch_agent.py            # Watch agent play
â”‚   â””â”€â”€ plot_results.py           # Plot training curves
â”œâ”€â”€ checkpoints/                  # Saved models (created during training)
â”œâ”€â”€ logs/                         # Training logs (created during training)
â””â”€â”€ requirements.txt              # Python dependencies
```

## ğŸš€ Getting Started

### 1. Environment Setup

#### Create Virtual Environment
```bash
python3.11 -m venv rl-cw-env
source rl-cw-env/bin/activate  # On Windows: rl-cw-env\Scripts\activate
```

#### Install Dependencies
```bash
pip install --upgrade pip
pip install -r requirements.txt
```

#### Install Atari ROMs
```bash
AutoROM --accept-license
```

### 2. Training the Agent

Train Rainbow DQN on Space Invaders:

```bash
python scripts/train.py
```

**Training Features:**
- Automatic checkpointing every 100 episodes
- Best model saved based on evaluation performance
- CSV logging for all metrics
- Optional TensorBoard logging
- Progress printed to console

**Training Configuration:**
All hyperparameters can be modified in `configs/space_invaders_config.py`:
- Network architecture (atoms, value range)
- Learning parameters (learning rate, discount factor)
- Buffer settings (capacity, prioritization)
- Training duration (episodes, steps)

### 3. Watching Your Agent Play

Watch the trained agent play Space Invaders with rendering:

```bash
# Watch best model
python scripts/watch_agent.py --checkpoint checkpoints/rainbow_space_invaders_best.pth --episodes 3

# Watch specific checkpoint
python scripts/watch_agent.py --checkpoint checkpoints/rainbow_space_invaders_ep500.pth --episodes 5
```

### 4. Evaluating Performance

Evaluate agent performance over many episodes (without rendering for speed):

```bash
# Evaluate best model over 100 episodes
python scripts/evaluate.py --checkpoint checkpoints/rainbow_space_invaders_best.pth --episodes 100
```

This will output comprehensive statistics:
- Mean return Â± standard deviation
- Median, min, and max returns
- Mean episode length

### 5. Visualizing Training Progress

Plot training curves from logs:

```bash
# Find your log file
ls logs/

# Plot training progress
python scripts/plot_results.py --csv logs/rainbow_space_invaders_TIMESTAMP.csv

# Save plot to file
python scripts/plot_results.py --csv logs/rainbow_space_invaders_TIMESTAMP.csv --save results.png
```

The plot includes:
- Episode returns (raw and smoothed)
- Episode lengths
- Training loss
- Buffer size and total steps

### 6. TensorBoard Visualization

If TensorBoard logging is enabled (default), you can view real-time training metrics:

```bash
tensorboard --logdir logs/tensorboard
```

Then open http://localhost:6006 in your browser.

## ğŸ® Environment Details

**Space Invaders Preprocessing:**
- **Frame stacking**: 4 consecutive frames
- **Grayscale conversion**: RGB â†’ grayscale
- **Frame skipping**: Action repeated for 4 frames
- **Reward clipping**: Rewards clipped to {-1, 0, +1}
- **Image size**: 84Ã—84 pixels
- **Normalization**: Pixel values scaled to [0, 1]

## ğŸ§  Network Architecture

**Input:** 4Ã—84Ã—84 (stacked grayscale frames)

**Convolutional Layers:**
- Conv1: 32 filters, 8Ã—8 kernel, stride 4
- Conv2: 64 filters, 4Ã—4 kernel, stride 2
- Conv3: 64 filters, 3Ã—3 kernel, stride 1

**Dueling Streams:**
- **Value Stream:** 512 hidden units â†’ 51 atoms
- **Advantage Stream:** 512 hidden units â†’ (num_actions Ã— 51 atoms)

**Output:** Distribution over 51 atoms for each action

**Special Features:**
- Noisy linear layers for exploration
- Distributional RL (C51) for learning value distributions

## âš™ï¸ Hyperparameters

Key hyperparameters (default values in `configs/space_invaders_config.py`):

| Parameter | Value | Description |
|-----------|-------|-------------|
| Learning Rate | 6.25e-5 | Adam optimizer learning rate |
| Discount Factor (Î³) | 0.99 | Reward discount |
| N-step | 3 | Steps for n-step returns |
| Target Update | 1000 steps | Target network update frequency |
| Batch Size | 32 | Training batch size |
| Buffer Size | 100,000 | Replay buffer capacity |
| Alpha | 0.6 | Prioritization exponent |
| Beta | 0.4 â†’ 1.0 | Importance sampling weight |
| Num Atoms | 51 | Distribution atoms (C51) |
| V_min / V_max | -10 / +10 | Value distribution support |

## ğŸ“Š Expected Performance

With default hyperparameters, you should see:
- **Training time**: ~8-12 hours on GPU (GTX 1080 or better)
- **Initial returns**: 100-200
- **After 500 episodes**: 400-600
- **After 1000 episodes**: 600-1000+
- **Human-level performance**: ~1000-1500

Performance may vary based on random seed and hardware.

## ğŸ”§ Customization

### Modifying Hyperparameters

Edit `configs/space_invaders_config.py` to change any hyperparameters:

```python
AGENT_CONFIG = {
    'learning_rate': 1e-4,  # Increase learning rate
    'gamma': 0.995,         # Higher discount factor
    'n_step': 5,            # More steps for n-step returns
}
```

### Training on Different Atari Games

Modify `ENV_CONFIG` in the config file:

```python
ENV_CONFIG = {
    'env_id': 'ALE/Breakout-v5',  # Or any other Atari game
    'frame_stack': 4,
    'image_size': 84,
}
```

### Adjusting Network Architecture

Modify `RainbowDQNNetwork` in `src/networks/rainbow_network.py` to change:
- Number of convolutional layers
- Filter sizes and counts
- Hidden layer sizes
- Number of atoms for C51

## ğŸ› Troubleshooting

### CUDA Out of Memory
- Reduce batch size in config
- Reduce buffer capacity
- Use CPU instead: set `DEVICE = 'cpu'` in config

### Slow Training
- Ensure CUDA is available: `torch.cuda.is_available()`
- Reduce logging frequency
- Disable TensorBoard if not needed

### ROM Not Found
```bash
AutoROM --accept-license
```

### Import Errors
Make sure all dependencies are installed:
```bash
pip install -r requirements.txt
```

## ğŸ“š References

**Original Papers:**
1. [Rainbow DQN](https://arxiv.org/abs/1710.02298) - Hessel et al., 2017
2. [DQN](https://arxiv.org/abs/1312.5602) - Mnih et al., 2013
3. [Double Q-Learning](https://arxiv.org/abs/1509.06461) - van Hasselt et al., 2015
4. [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952) - Schaul et al., 2015
5. [Dueling DQN](https://arxiv.org/abs/1511.06581) - Wang et al., 2015
6. [Noisy Networks](https://arxiv.org/abs/1706.10295) - Fortunato et al., 2017
7. [Distributional RL](https://arxiv.org/abs/1707.06887) - Bellemare et al., 2017

## ğŸ“ License

This project is for educational purposes as part of University of Bath Reinforcement Learning coursework.

## ğŸ™ Acknowledgments

- OpenAI Gymnasium for Atari environments
- PyTorch team for the deep learning framework
- Original Rainbow DQN authors

---

**Happy Training! ğŸ®ğŸ¤–**
