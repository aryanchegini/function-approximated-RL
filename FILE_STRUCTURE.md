# Rainbow DQN Project Structure

## ğŸ“ Complete File Structure

```
RLCoursework/
â”‚
â”œâ”€â”€ ğŸ“„ README_RAINBOW.md          # Main documentation (comprehensive guide)
â”œâ”€â”€ ğŸ“„ PROJECT_SUMMARY.md         # Project overview and what's implemented
â”œâ”€â”€ ğŸ“„ PIPELINE_OVERVIEW.md       # Detailed pipeline and architecture diagrams
â”œâ”€â”€ ğŸ“„ FILE_STRUCTURE.md          # This file - explains each file
â”œâ”€â”€ ğŸ“„ QUICKSTART.py              # Quick reference for common commands
â”œâ”€â”€ ğŸ“„ test_installation.py       # Test script to verify setup
â”œâ”€â”€ ğŸ“„ requirements.txt           # Python dependencies
â”œâ”€â”€ ğŸ“„ .gitignore                 # Git ignore rules
â”‚
â”œâ”€â”€ ğŸ“ src/                       # Source code (all from scratch)
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ agents/                # Agent implementations
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ rainbow_agent.py   # Rainbow DQN agent
â”‚   â”‚       â”œâ”€â”€ RainbowDQNAgent class
â”‚   â”‚       â”œâ”€â”€ Double Q-learning
â”‚   â”‚       â”œâ”€â”€ Distributional RL (C51)
â”‚   â”‚       â”œâ”€â”€ Target network updates
â”‚   â”‚       â”œâ”€â”€ Model save/load
â”‚   â”‚       â””â”€â”€ Training/eval modes
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ networks/              # Neural network architectures
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ noisy_linear.py    # Noisy linear layer
â”‚   â”‚   â”‚   â”œâ”€â”€ NoisyLinear class
â”‚   â”‚   â”‚   â”œâ”€â”€ Factorized Gaussian noise
â”‚   â”‚   â”‚   â”œâ”€â”€ Parameter initialization
â”‚   â”‚   â”‚   â””â”€â”€ Noise reset mechanism
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ ğŸ“„ rainbow_network.py # Rainbow DQN network
â”‚   â”‚       â”œâ”€â”€ RainbowDQNNetwork class
â”‚   â”‚       â”œâ”€â”€ Conv feature extractor (Nature DQN)
â”‚   â”‚       â”œâ”€â”€ Dueling architecture (value + advantage)
â”‚   â”‚       â”œâ”€â”€ Noisy layers for exploration
â”‚   â”‚       â”œâ”€â”€ C51 distributional output (51 atoms)
â”‚   â”‚       â””â”€â”€ Q-value computation from distributions
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ environment/           # Environment wrappers
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ atari_wrappers.py  # Atari preprocessing
â”‚   â”‚       â”œâ”€â”€ NoopResetEnv (random initial states)
â”‚   â”‚       â”œâ”€â”€ MaxAndSkipEnv (4-frame skip + max pool)
â”‚   â”‚       â”œâ”€â”€ FireResetEnv (auto-fire at start)
â”‚   â”‚       â”œâ”€â”€ EpisodicLifeEnv (life loss = episode end)
â”‚   â”‚       â”œâ”€â”€ ClipRewardEnv (clip to {-1, 0, +1})
â”‚   â”‚       â”œâ”€â”€ WarpFrame (84x84 grayscale)
â”‚   â”‚       â”œâ”€â”€ FrameStack (stack 4 frames)
â”‚   â”‚       â”œâ”€â”€ ScaledFloatFrame (normalize to [0, 1])
â”‚   â”‚       â””â”€â”€ make_atari_env() helper function
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ utils/                 # Utility functions
â”‚       â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â”œâ”€â”€ ğŸ“„ replay_buffer.py   # Prioritized replay buffer
â”‚       â”‚   â”œâ”€â”€ SumTree class (efficient prioritized sampling)
â”‚       â”‚   â”œâ”€â”€ PrioritizedReplayBuffer class
â”‚       â”‚   â”œâ”€â”€ Add experiences with priorities
â”‚       â”‚   â”œâ”€â”€ Sample batch (prioritized)
â”‚       â”‚   â”œâ”€â”€ Update priorities (TD errors)
â”‚       â”‚   â””â”€â”€ Beta annealing for importance sampling
â”‚       â”‚
â”‚       â”œâ”€â”€ ğŸ“„ n_step.py           # N-step returns
â”‚       â”‚   â”œâ”€â”€ NStepBuffer class
â”‚       â”‚   â”œâ”€â”€ Accumulate n-step rewards
â”‚       â”‚   â”œâ”€â”€ Handle episode boundaries
â”‚       â”‚   â””â”€â”€ Compute n-step returns
â”‚       â”‚
â”‚       â””â”€â”€ ğŸ“„ logger.py           # Logging utilities
â”‚           â”œâ”€â”€ Logger class (CSV logging)
â”‚           â”œâ”€â”€ TensorBoardLogger class
â”‚           â”œâ”€â”€ Episode metrics logging
â”‚           â””â”€â”€ Console output formatting
â”‚
â”œâ”€â”€ ğŸ“ configs/                   # Configuration files
â”‚   â””â”€â”€ ğŸ“„ space_invaders_config.py  # All hyperparameters
â”‚       â”œâ”€â”€ ENV_CONFIG (environment settings)
â”‚       â”œâ”€â”€ AGENT_CONFIG (network & learning params)
â”‚       â”œâ”€â”€ BUFFER_CONFIG (replay buffer settings)
â”‚       â”œâ”€â”€ TRAINING_CONFIG (training duration & frequency)
â”‚       â”œâ”€â”€ LOGGING_CONFIG (log paths & options)
â”‚       â”œâ”€â”€ DEVICE (cuda/cpu)
â”‚       â””â”€â”€ SEED (reproducibility)
â”‚
â”œâ”€â”€ ğŸ“ scripts/                   # Executable scripts
â”‚   â”œâ”€â”€ ğŸ“„ train.py               # Main training script
â”‚   â”‚   â”œâ”€â”€ Setup environment & agent
â”‚   â”‚   â”œâ”€â”€ Initialize replay buffer
â”‚   â”‚   â”œâ”€â”€ Training loop
â”‚   â”‚   â”œâ”€â”€ Logging (CSV + TensorBoard)
â”‚   â”‚   â”œâ”€â”€ Periodic evaluation
â”‚   â”‚   â”œâ”€â”€ Checkpointing (regular + best)
â”‚   â”‚   â””â”€â”€ Progress monitoring
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“„ watch_agent.py         # Watch agent play (with rendering)
â”‚   â”‚   â”œâ”€â”€ Load trained model
â”‚   â”‚   â”œâ”€â”€ Play N episodes with rendering
â”‚   â”‚   â””â”€â”€ Display performance stats
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“„ evaluate.py            # Evaluate agent performance
â”‚   â”‚   â”œâ”€â”€ Load trained model
â”‚   â”‚   â”œâ”€â”€ Run N episodes (no rendering)
â”‚   â”‚   â”œâ”€â”€ Collect statistics
â”‚   â”‚   â””â”€â”€ Report mean/std/min/max returns
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“„ plot_results.py        # Visualize training progress
â”‚       â”œâ”€â”€ Load CSV logs
â”‚       â”œâ”€â”€ Plot episode returns (raw & smoothed)
â”‚       â”œâ”€â”€ Plot episode lengths
â”‚       â”œâ”€â”€ Plot training loss
â”‚       â”œâ”€â”€ Plot buffer size & steps
â”‚       â””â”€â”€ Save/display plots
â”‚
â”œâ”€â”€ ğŸ“ checkpoints/               # Saved models (created during training)
â”‚   â”œâ”€â”€ rainbow_space_invaders_best.pth      # Best performing model
â”‚   â”œâ”€â”€ rainbow_space_invaders_final.pth     # Final model after training
â”‚   â””â”€â”€ rainbow_space_invaders_ep{N}.pth     # Checkpoint every 100 episodes
â”‚
â””â”€â”€ ğŸ“ logs/                      # Training logs (created during training)
    â”œâ”€â”€ rainbow_space_invaders_TIMESTAMP.csv  # CSV metrics log
    â””â”€â”€ tensorboard/              # TensorBoard event files
        â””â”€â”€ rainbow_space_invaders_TIMESTAMP/
```

## ğŸ” Key File Descriptions

### Core Implementation Files (Most Important)

#### `src/networks/rainbow_network.py`
**Purpose**: Complete Rainbow DQN network architecture  
**Key Features**:
- Nature DQN convolutional layers
- Dueling architecture (separate value and advantage streams)
- Distributional RL (outputs distribution over 51 atoms)
- Noisy layers for exploration
- ~2.5M parameters

#### `src/agents/rainbow_agent.py`
**Purpose**: Rainbow DQN agent with full learning algorithm  
**Key Features**:
- Double Q-learning (action selection vs evaluation)
- Categorical projection for distributional Bellman update
- N-step bootstrapping
- Target network management
- Model save/load functionality
- Training/evaluation mode switching

#### `src/utils/replay_buffer.py`
**Purpose**: Prioritized experience replay  
**Key Features**:
- Sum tree data structure (O(log n) operations)
- Priority-based sampling (alpha parameter)
- Importance sampling weights (beta annealing)
- Automatic priority updates based on TD errors

#### `src/networks/noisy_linear.py`
**Purpose**: Noisy networks for exploration  
**Key Features**:
- Factorized Gaussian noise
- Learnable noise parameters (mu and sigma)
- Training vs eval mode (noise on/off)
- Replaces epsilon-greedy exploration

#### `src/environment/atari_wrappers.py`
**Purpose**: Standard Atari preprocessing  
**Key Features**:
- Frame skipping and max pooling
- Grayscale conversion and resizing
- Reward clipping
- Frame stacking
- Episodic life management

### Training & Evaluation Scripts

#### `scripts/train.py`
**Purpose**: Main training loop  
**Usage**: `python scripts/train.py`  
**Features**:
- Complete training pipeline
- Automatic checkpointing
- CSV and TensorBoard logging
- Periodic evaluation
- Best model tracking

#### `scripts/watch_agent.py`
**Purpose**: Visualize trained agent  
**Usage**: `python scripts/watch_agent.py --checkpoint path/to/model.pth --episodes 3`  
**Features**:
- Load any checkpoint
- Render gameplay
- Display episode returns

#### `scripts/evaluate.py`
**Purpose**: Statistical performance evaluation  
**Usage**: `python scripts/evaluate.py --checkpoint path/to/model.pth --episodes 100`  
**Features**:
- Fast evaluation (no rendering)
- Comprehensive statistics
- Mean Â± std, median, min/max

#### `scripts/plot_results.py`
**Purpose**: Visualize training progress  
**Usage**: `python scripts/plot_results.py --csv logs/file.csv --save plot.png`  
**Features**:
- 4-panel plot (returns, length, loss, buffer/steps)
- Smoothing for clarity
- Save or display

### Configuration & Documentation

#### `configs/space_invaders_config.py`
**Purpose**: Central configuration  
**Contains**:
- All hyperparameters
- Environment settings
- Training parameters
- Easy to modify for experiments

#### `README_RAINBOW.md`
**Purpose**: Main documentation  
**Contains**:
- Getting started guide
- Complete usage instructions
- Hyperparameter explanations
- Troubleshooting
- References

#### `PROJECT_SUMMARY.md`
**Purpose**: Implementation overview  
**Contains**:
- What's implemented
- Architecture details
- Expected results
- Key features

#### `PIPELINE_OVERVIEW.md`
**Purpose**: Visual pipeline explanation  
**Contains**:
- ASCII diagrams of architecture
- Training loop flow
- Data flow diagrams
- Learning algorithm details

### Utility Files

#### `test_installation.py`
**Purpose**: Verify setup  
**Usage**: `python test_installation.py`  
**Tests**:
- All dependencies installed
- Environment creation
- Custom modules importable
- Agent creation
- Forward pass

#### `requirements.txt`
**Purpose**: Python dependencies  
**Usage**: `pip install -r requirements.txt`  
**Contains**:
- PyTorch
- Gymnasium with Atari
- OpenCV
- Plotting libraries
- TensorBoard

#### `QUICKSTART.py`
**Purpose**: Quick reference  
**Contains**:
- Common commands
- Usage examples
- Configuration tips
- Expected performance

## ğŸ¯ Most Important Files to Understand

For learning Rainbow DQN, study these in order:

1. **`configs/space_invaders_config.py`** - See all hyperparameters
2. **`src/networks/noisy_linear.py`** - Understand noisy networks
3. **`src/networks/rainbow_network.py`** - See full architecture
4. **`src/utils/replay_buffer.py`** - Understand prioritized replay
5. **`src/utils/n_step.py`** - See n-step returns
6. **`src/agents/rainbow_agent.py`** - Complete learning algorithm
7. **`src/environment/atari_wrappers.py`** - Environment preprocessing
8. **`scripts/train.py`** - See it all come together

## ğŸ”§ Files to Modify for Experimentation

- **Change hyperparameters**: `configs/space_invaders_config.py`
- **Modify network**: `src/networks/rainbow_network.py`
- **Adjust training loop**: `scripts/train.py`
- **Try different game**: Change `env_id` in config

## ğŸ“Š Output Files (Generated During Training)

- **Checkpoints**: `checkpoints/*.pth` - Saved models
- **CSV logs**: `logs/*.csv` - Training metrics
- **TensorBoard**: `logs/tensorboard/` - Real-time visualization
- **Plots**: Generated by `plot_results.py`

---

**Total Lines of Code**: ~2,500 lines  
**Total Files**: 20+ files  
**Implementation**: 100% from scratch  
**Status**: Production ready âœ…
